{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_blsSAIy2y12",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "# Agents in LlamaIndex\n",
        "\n",
        "## Let's install the dependencies\n",
        "\n",
        "We will install the dependencies for this unit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wb_ea0Br2y2B",
        "outputId": "eab7168a-a5e0-4663-f367-6cd58bdbd75e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.2/313.2 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.1/137.1 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install llama-index llama-index-vector-stores-chroma llama-index-llms-huggingface-api llama-index-embeddings-huggingface -U -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9Pfr_dd2y2F"
      },
      "source": [
        "And, let's log in to Hugging Face to use serverless Inference APIs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "sJiWOgwN2y2G",
        "outputId": "1d707708-2698-4eec-f0e9-0ff048e09886"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_3W3Dl02y2H",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "## Initialising agents\n",
        "\n",
        "Let's start by initialising an agent. We will use the basic `AgentWorkflow` class to create an agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zrec0SOQ2y2H"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
        "from llama_index.core.agent.workflow import AgentWorkflow, ToolCallResult, AgentStream\n",
        "\n",
        "\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Add two numbers\"\"\"\n",
        "    return a + b\n",
        "\n",
        "\n",
        "def subtract(a: int, b: int) -> int:\n",
        "    \"\"\"Subtract two numbers\"\"\"\n",
        "    return a - b\n",
        "\n",
        "\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply two numbers\"\"\"\n",
        "    return a * b\n",
        "\n",
        "\n",
        "def divide(a: int, b: int) -> int:\n",
        "    \"\"\"Divide two numbers\"\"\"\n",
        "    return a / b\n",
        "\n",
        "\n",
        "# llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
        "llm = HuggingFaceInferenceAPI(model_name=\"HuggingFaceTB/SmolLM3-3B\")\n",
        "\n",
        "\n",
        "agent = AgentWorkflow.from_tools_or_functions(\n",
        "    tools_or_functions=[subtract, multiply, divide, add],\n",
        "    llm=llm,\n",
        "    system_prompt=\"You are a math agent that can add, subtract, multiply, and divide numbers using provided tools.\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXIUXEKr2y2J"
      },
      "source": [
        "Then, we can run the agent and get the response and reasoning behind the tool calls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XW6eZizz2y2K",
        "outputId": "405771e6-19de-43fe-d309-0323ead49d0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<think>\n",
            "Okay, let's see. The user is asking for the result of (2 + 2) multiplied by 2. First, I need to break this down step by step. According to the order of operations, I should handle the addition first before the multiplication. So, 2 plus 2 equals 4. Then, take that result and multiply it by 2. 4 times 2 is 8. I don't need any tools for this since it's a straightforward arithmetic calculation. I can just compute it manually. The answer should be 8. Let me make sure I didn't miss any steps. Yeah, addition first, then multiplication. Yep, that's right. So I can confidently say the result is 8 without using any tools.\n",
            "</think>\n",
            "\n",
            "Thought: I can answer without using any more tools. I'll use the user's language to answer  \n",
            "Answer: 8"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-67' coro=<<async_generator_athrow without __name__>()> exception=KeyError(<aiohttp.client.ClientSession object at 0x7dc442d4ab90>)>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 379, in _async_yield_from\n",
            "    yield byte_payload.strip()\n",
            "GeneratorExit\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 382, in _async_yield_from\n",
            "    await client.close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_generated/_async_client.py\", line 3389, in close_session\n",
            "    for response in self._sessions[session]:\n",
            "                    ~~~~~~~~~~~~~~^^^^^^^^^\n",
            "KeyError: <aiohttp.client.ClientSession object at 0x7dc442d4ab90>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AgentOutput(response=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='8')]), structured_response=None, current_agent_name='Agent', raw=ChatCompletionStreamOutput(choices=[ChatCompletionStreamOutputChoice(delta=ChatCompletionStreamOutputDelta(role=None, content='8', tool_call_id=None, tool_calls=None), index=0, finish_reason=None, logprobs=None)], created=1753958690, id='chatcmpl-Root=1-688b4922-4f4a65a849788b770a930c6e', model='HuggingFaceTB/SmolLM3-3B', system_fingerprint=None, usage=None, object='chat.completion.chunk'), tool_calls=[], retry_messages=[])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "handler = agent.run(\"What is (2 + 2) * 2?\")\n",
        "async for ev in handler.stream_events():\n",
        "    if isinstance(ev, ToolCallResult):\n",
        "        print(\"\")\n",
        "        print(\"Called tool: \", ev.tool_name, ev.tool_kwargs, \"=>\", ev.tool_output)\n",
        "    elif isinstance(ev, AgentStream):  # showing the thought process\n",
        "        print(ev.delta, end=\"\", flush=True)\n",
        "\n",
        "resp = await handler\n",
        "resp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmVWAb5o2y2L"
      },
      "source": [
        "In a similar fashion, we can pass state and context to the agent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fULfqw712y2M",
        "outputId": "94fdd09b-1100-4634-daab-cb14822cb783"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-281' coro=<<async_generator_athrow without __name__>()> exception=KeyError(<aiohttp.client.ClientSession object at 0x7dc442880e90>)>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 379, in _async_yield_from\n",
            "    yield byte_payload.strip()\n",
            "GeneratorExit\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 382, in _async_yield_from\n",
            "    await client.close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_generated/_async_client.py\", line 3389, in close_session\n",
            "    for response in self._sessions[session]:\n",
            "                    ~~~~~~~~~~~~~~^^^^^^^^^\n",
            "KeyError: <aiohttp.client.ClientSession object at 0x7dc442880e90>\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-314' coro=<<async_generator_athrow without __name__>()> exception=KeyError(<aiohttp.client.ClientSession object at 0x7dc4427876d0>)>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 379, in _async_yield_from\n",
            "    yield byte_payload.strip()\n",
            "GeneratorExit\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 382, in _async_yield_from\n",
            "    await client.close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_generated/_async_client.py\", line 3389, in close_session\n",
            "    for response in self._sessions[session]:\n",
            "                    ~~~~~~~~~~~~~~^^^^^^^^^\n",
            "KeyError: <aiohttp.client.ClientSession object at 0x7dc4427876d0>\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-320' coro=<<async_generator_athrow without __name__>()> exception=KeyError(<aiohttp.client.ClientSession object at 0x7dc442810e50>)>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 379, in _async_yield_from\n",
            "    yield byte_payload.strip()\n",
            "GeneratorExit\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 382, in _async_yield_from\n",
            "    await client.close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_generated/_async_client.py\", line 3389, in close_session\n",
            "    for response in self._sessions[session]:\n",
            "                    ~~~~~~~~~~~~~~^^^^^^^^^\n",
            "KeyError: <aiohttp.client.ClientSession object at 0x7dc442810e50>\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-326' coro=<<async_generator_athrow without __name__>()> exception=KeyError(<aiohttp.client.ClientSession object at 0x7dc442621550>)>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 379, in _async_yield_from\n",
            "    yield byte_payload.strip()\n",
            "GeneratorExit\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 382, in _async_yield_from\n",
            "    await client.close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_generated/_async_client.py\", line 3389, in close_session\n",
            "    for response in self._sessions[session]:\n",
            "                    ~~~~~~~~~~~~~~^^^^^^^^^\n",
            "KeyError: <aiohttp.client.ClientSession object at 0x7dc442621550>\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-332' coro=<<async_generator_athrow without __name__>()> exception=KeyError(<aiohttp.client.ClientSession object at 0x7dc442644c10>)>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 379, in _async_yield_from\n",
            "    yield byte_payload.strip()\n",
            "GeneratorExit\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 382, in _async_yield_from\n",
            "    await client.close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_generated/_async_client.py\", line 3389, in close_session\n",
            "    for response in self._sessions[session]:\n",
            "                    ~~~~~~~~~~~~~~^^^^^^^^^\n",
            "KeyError: <aiohttp.client.ClientSession object at 0x7dc442644c10>\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-338' coro=<<async_generator_athrow without __name__>()> exception=KeyError(<aiohttp.client.ClientSession object at 0x7dc44277c2d0>)>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 379, in _async_yield_from\n",
            "    yield byte_payload.strip()\n",
            "GeneratorExit\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 382, in _async_yield_from\n",
            "    await client.close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_generated/_async_client.py\", line 3389, in close_session\n",
            "    for response in self._sessions[session]:\n",
            "                    ~~~~~~~~~~~~~~^^^^^^^^^\n",
            "KeyError: <aiohttp.client.ClientSession object at 0x7dc44277c2d0>\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-344' coro=<<async_generator_athrow without __name__>()> exception=KeyError(<aiohttp.client.ClientSession object at 0x7dc442699d10>)>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 379, in _async_yield_from\n",
            "    yield byte_payload.strip()\n",
            "GeneratorExit\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 382, in _async_yield_from\n",
            "    await client.close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_generated/_async_client.py\", line 3389, in close_session\n",
            "    for response in self._sessions[session]:\n",
            "                    ~~~~~~~~~~~~~~^^^^^^^^^\n",
            "KeyError: <aiohttp.client.ClientSession object at 0x7dc442699d10>\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-350' coro=<<async_generator_athrow without __name__>()> exception=KeyError(<aiohttp.client.ClientSession object at 0x7dc4426c4cd0>)>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 379, in _async_yield_from\n",
            "    yield byte_payload.strip()\n",
            "GeneratorExit\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 382, in _async_yield_from\n",
            "    await client.close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_generated/_async_client.py\", line 3389, in close_session\n",
            "    for response in self._sessions[session]:\n",
            "                    ~~~~~~~~~~~~~~^^^^^^^^^\n",
            "KeyError: <aiohttp.client.ClientSession object at 0x7dc4426c4cd0>\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-356' coro=<<async_generator_athrow without __name__>()> exception=KeyError(<aiohttp.client.ClientSession object at 0x7dc4424f2590>)>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 379, in _async_yield_from\n",
            "    yield byte_payload.strip()\n",
            "GeneratorExit\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 382, in _async_yield_from\n",
            "    await client.close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_generated/_async_client.py\", line 3389, in close_session\n",
            "    for response in self._sessions[session]:\n",
            "                    ~~~~~~~~~~~~~~^^^^^^^^^\n",
            "KeyError: <aiohttp.client.ClientSession object at 0x7dc4424f2590>\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-362' coro=<<async_generator_athrow without __name__>()> exception=KeyError(<aiohttp.client.ClientSession object at 0x7dc4424f3f90>)>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 379, in _async_yield_from\n",
            "    yield byte_payload.strip()\n",
            "GeneratorExit\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 382, in _async_yield_from\n",
            "    await client.close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_generated/_async_client.py\", line 3389, in close_session\n",
            "    for response in self._sessions[session]:\n",
            "                    ~~~~~~~~~~~~~~^^^^^^^^^\n",
            "KeyError: <aiohttp.client.ClientSession object at 0x7dc4424f3f90>\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-368' coro=<<async_generator_athrow without __name__>()> exception=KeyError(<aiohttp.client.ClientSession object at 0x7dc4426c4b50>)>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 379, in _async_yield_from\n",
            "    yield byte_payload.strip()\n",
            "GeneratorExit\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 382, in _async_yield_from\n",
            "    await client.close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_generated/_async_client.py\", line 3389, in close_session\n",
            "    for response in self._sessions[session]:\n",
            "                    ~~~~~~~~~~~~~~^^^^^^^^^\n",
            "KeyError: <aiohttp.client.ClientSession object at 0x7dc4426c4b50>\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-374' coro=<<async_generator_athrow without __name__>()> exception=KeyError(<aiohttp.client.ClientSession object at 0x7dc4425865d0>)>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 379, in _async_yield_from\n",
            "    yield byte_payload.strip()\n",
            "GeneratorExit\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 382, in _async_yield_from\n",
            "    await client.close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_generated/_async_client.py\", line 3389, in close_session\n",
            "    for response in self._sessions[session]:\n",
            "                    ~~~~~~~~~~~~~~^^^^^^^^^\n",
            "KeyError: <aiohttp.client.ClientSession object at 0x7dc4425865d0>\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-380' coro=<<async_generator_athrow without __name__>()> exception=KeyError(<aiohttp.client.ClientSession object at 0x7dc4425b3690>)>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 379, in _async_yield_from\n",
            "    yield byte_payload.strip()\n",
            "GeneratorExit\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 382, in _async_yield_from\n",
            "    await client.close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_generated/_async_client.py\", line 3389, in close_session\n",
            "    for response in self._sessions[session]:\n",
            "                    ~~~~~~~~~~~~~~^^^^^^^^^\n",
            "KeyError: <aiohttp.client.ClientSession object at 0x7dc4425b3690>\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-386' coro=<<async_generator_athrow without __name__>()> exception=KeyError(<aiohttp.client.ClientSession object at 0x7dc4423edd90>)>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 379, in _async_yield_from\n",
            "    yield byte_payload.strip()\n",
            "GeneratorExit\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 382, in _async_yield_from\n",
            "    await client.close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_generated/_async_client.py\", line 3389, in close_session\n",
            "    for response in self._sessions[session]:\n",
            "                    ~~~~~~~~~~~~~~^^^^^^^^^\n",
            "KeyError: <aiohttp.client.ClientSession object at 0x7dc4423edd90>\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-392' coro=<<async_generator_athrow without __name__>()> exception=KeyError(<aiohttp.client.ClientSession object at 0x7dc442419790>)>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 379, in _async_yield_from\n",
            "    yield byte_payload.strip()\n",
            "GeneratorExit\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 382, in _async_yield_from\n",
            "    await client.close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_generated/_async_client.py\", line 3389, in close_session\n",
            "    for response in self._sessions[session]:\n",
            "                    ~~~~~~~~~~~~~~^^^^^^^^^\n",
            "KeyError: <aiohttp.client.ClientSession object at 0x7dc442419790>\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-398' coro=<<async_generator_athrow without __name__>()> exception=KeyError(<aiohttp.client.ClientSession object at 0x7dc44241b690>)>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 379, in _async_yield_from\n",
            "    yield byte_payload.strip()\n",
            "GeneratorExit\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 382, in _async_yield_from\n",
            "    await client.close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_generated/_async_client.py\", line 3389, in close_session\n",
            "    for response in self._sessions[session]:\n",
            "                    ~~~~~~~~~~~~~~^^^^^^^^^\n",
            "KeyError: <aiohttp.client.ClientSession object at 0x7dc44241b690>\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-404' coro=<<async_generator_athrow without __name__>()> exception=KeyError(<aiohttp.client.ClientSession object at 0x7dc442458890>)>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 379, in _async_yield_from\n",
            "    yield byte_payload.strip()\n",
            "GeneratorExit\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 382, in _async_yield_from\n",
            "    await client.close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_generated/_async_client.py\", line 3389, in close_session\n",
            "    for response in self._sessions[session]:\n",
            "                    ~~~~~~~~~~~~~~^^^^^^^^^\n",
            "KeyError: <aiohttp.client.ClientSession object at 0x7dc442458890>\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-410' coro=<<async_generator_athrow without __name__>()> exception=KeyError(<aiohttp.client.ClientSession object at 0x7dc4424abed0>)>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 379, in _async_yield_from\n",
            "    yield byte_payload.strip()\n",
            "GeneratorExit\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 382, in _async_yield_from\n",
            "    await client.close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_generated/_async_client.py\", line 3389, in close_session\n",
            "    for response in self._sessions[session]:\n",
            "                    ~~~~~~~~~~~~~~^^^^^^^^^\n",
            "KeyError: <aiohttp.client.ClientSession object at 0x7dc4424abed0>\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-416' coro=<<async_generator_athrow without __name__>()> exception=KeyError(<aiohttp.client.ClientSession object at 0x7dc4422f63d0>)>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 379, in _async_yield_from\n",
            "    yield byte_payload.strip()\n",
            "GeneratorExit\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 382, in _async_yield_from\n",
            "    await client.close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_generated/_async_client.py\", line 3389, in close_session\n",
            "    for response in self._sessions[session]:\n",
            "                    ~~~~~~~~~~~~~~^^^^^^^^^\n",
            "KeyError: <aiohttp.client.ClientSession object at 0x7dc4422f63d0>\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-422' coro=<<async_generator_athrow without __name__>()> exception=KeyError(<aiohttp.client.ClientSession object at 0x7dc442325d90>)>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 379, in _async_yield_from\n",
            "    yield byte_payload.strip()\n",
            "GeneratorExit\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 382, in _async_yield_from\n",
            "    await client.close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_generated/_async_client.py\", line 3389, in close_session\n",
            "    for response in self._sessions[session]:\n",
            "                    ~~~~~~~~~~~~~~^^^^^^^^^\n",
            "KeyError: <aiohttp.client.ClientSession object at 0x7dc442325d90>\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-428' coro=<<async_generator_athrow without __name__>()> exception=KeyError(<aiohttp.client.ClientSession object at 0x7dc44235a250>)>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 379, in _async_yield_from\n",
            "    yield byte_payload.strip()\n",
            "GeneratorExit\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 382, in _async_yield_from\n",
            "    await client.close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_generated/_async_client.py\", line 3389, in close_session\n",
            "    for response in self._sessions[session]:\n",
            "                    ~~~~~~~~~~~~~~^^^^^^^^^\n",
            "KeyError: <aiohttp.client.ClientSession object at 0x7dc44235a250>\n"
          ]
        },
        {
          "ename": "WorkflowRuntimeError",
          "evalue": "Error in step 'parse_agent_output': Max iterations of 20 reached! Either something went wrong, or you can increase the max iterations with `.run(.., max_iterations=...)`",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mWorkflowRuntimeError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/workflows/context/context.py\u001b[0m in \u001b[0;36m_step_worker\u001b[0;34m(self, name, step, config, stepwise, verbose, checkpoint_callback, run_id, service_manager, resource_manager)\u001b[0m\n\u001b[1;32m    688\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m                         \u001b[0mnew_ev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0minstrumented_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m                         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index_instrumentation/dispatcher.py\u001b[0m in \u001b[0;36masync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/agent/workflow/multi_agent_workflow.py\u001b[0m in \u001b[0;36mparse_agent_output\u001b[0;34m(self, ctx, ev)\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnum_iterations\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmax_iterations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m             raise WorkflowRuntimeError(\n\u001b[0m\u001b[1;32m    434\u001b[0m                 \u001b[0;34mf\"Max iterations of {max_iterations} reached! Either something went wrong, or you can \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mWorkflowRuntimeError\u001b[0m: Max iterations of 20 reached! Either something went wrong, or you can increase the max iterations with `.run(.., max_iterations=...)`",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mWorkflowRuntimeError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-118986801.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"My name is Bob.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"What was my name again?\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/tasks.py\u001b[0m in \u001b[0;36m__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__wakeup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m             \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0;31m# This may also be a cancellation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/workflows/workflow.py\u001b[0m in \u001b[0;36m_run_workflow\u001b[0;34m()\u001b[0m\n\u001b[1;32m    414\u001b[0m                     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_event_to_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStopEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mexception_raised\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwe_done\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    275\u001b[0m                 \u001b[0;31m# We use the `send` method directly, because coroutines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;31m# don't have `__iter__` and `__next__` methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/workflows/context/context.py\u001b[0m in \u001b[0;36m_step_worker\u001b[0;34m(self, name, step, config, stepwise, verbose, checkpoint_callback, run_id, service_manager, resource_manager)\u001b[0m\n\u001b[1;32m    696\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry_policy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m                             raise WorkflowRuntimeError(\n\u001b[0m\u001b[1;32m    699\u001b[0m                                 \u001b[0;34mf\"Error in step '{name}': {e!s}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                             ) from e\n",
            "\u001b[0;31mWorkflowRuntimeError\u001b[0m: Error in step 'parse_agent_output': Max iterations of 20 reached! Either something went wrong, or you can increase the max iterations with `.run(.., max_iterations=...)`"
          ]
        }
      ],
      "source": [
        "from llama_index.core.workflow import Context\n",
        "\n",
        "ctx = Context(agent)\n",
        "\n",
        "response = await agent.run(\"My name is Bob.\", ctx=ctx)\n",
        "response = await agent.run(\"What was my name again?\", ctx=ctx)\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvcLBDo92y2O"
      },
      "source": [
        "## Creating RAG Agents with QueryEngineTools\n",
        "\n",
        "Let's now re-use the `QueryEngine` we defined in the [previous unit on tools](/tools.ipynb) and convert it into a `QueryEngineTool`. We will pass it to the `AgentWorkflow` class to create a RAG agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113
        },
        "id": "mtDHeUo1G5Ns",
        "outputId": "e66149d1-96d7-4b8f-e11f-b956ec7be3a8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/618 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "data/train-00000-of-00001.parquet:   0%|          | 0.00/35.0M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from pathlib import Path\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "dataset = load_dataset(path=\"dvilasuero/finepersonas-v0.1-tiny\", split=\"train\")\n",
        "\n",
        "Path(\"data\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for i, persona in enumerate(dataset):\n",
        "    with open(Path(\"data\") / f\"persona_{i}.txt\", \"w\") as f:\n",
        "        f.write(persona[\"persona\"])\n",
        "\n",
        "\n",
        "reader = SimpleDirectoryReader(input_dir=\"data\")\n",
        "documents = reader.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "SRuAmfvV2y2P",
        "outputId": "d7d220fa-5d18-4dfc-9ec8-3b20bc69ff51"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "import chromadb\n",
        "\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "\n",
        "# Create a vector store\n",
        "db = chromadb.PersistentClient(path=\"./alfred_chroma_db\")\n",
        "chroma_collection = db.get_or_create_collection(\"alfred\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "\n",
        "# Create a query engine\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "# llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
        "llm = HuggingFaceInferenceAPI(model_name=\"HuggingFaceTB/SmolLM3-3B\")\n",
        "\n",
        "# index = VectorStoreIndex.from_vector_store(\n",
        "#     vector_store=vector_store, embed_model=embed_model\n",
        "# )\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    embed_model=embed_model,\n",
        "    vector_store=vector_store\n",
        ")\n",
        "\n",
        "query_engine = index.as_query_engine(llm=llm)\n",
        "query_engine_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=query_engine,\n",
        "    name=\"personas\",\n",
        "    description=\"descriptions for various types of personas\",\n",
        "    return_direct=False,\n",
        ")\n",
        "\n",
        "# Create a RAG agent\n",
        "query_engine_agent = AgentWorkflow.from_tools_or_functions(\n",
        "    tools_or_functions=[query_engine_tool],\n",
        "    llm=llm,\n",
        "    system_prompt=\"You are a helpful assistant that has access to a database containing persona descriptions. \",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOiH3YKg2y2Q"
      },
      "source": [
        "And, we can once more get the response and reasoning behind the tool calls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzyxqJCF2y2R",
        "outputId": "b0ad97bb-eb39-4652-dd57-56d6eb2549e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<think>\n",
            "Okay, the user wants me to search a database for 'science fiction' and return some persona descriptions. Let me start by understanding what they need. They mentioned using a tool called personas, which I assume is a tool that provides descriptions for different types of personas. \n",
            "\n",
            "First, I need to check if the tool is available. The tool name is personas, and the description says it provides descriptions for various types of personas. The required input is a title, which in this case is 'science fiction'. So I should use the personas tool with the input 'science fiction'.\n",
            "\n",
            "Next, I need to make sure I'm using the correct tool and input. The user might be referring to a specific database, but since they didn't specify, I'll proceed with the assumption that the personas tool is designed to handle such queries. \n",
            "\n",
            "I should also consider if there are any additional parameters needed. The tool's description mentions a properties object with input and type, so the input should be a string titled 'Input'. The required field is 'input', so the JSON should look like {\"input\": \"science fiction\"}.\n",
            "\n",
            "After running the tool, I'll need to process the response. The tool might return a list of persona descriptions related to science fiction. Each description would"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-462' coro=<<async_generator_athrow without __name__>()> exception=KeyError(<aiohttp.client.ClientSession object at 0x7dc2b7dcbd10>)>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 379, in _async_yield_from\n",
            "    yield byte_payload.strip()\n",
            "GeneratorExit\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\", line 382, in _async_yield_from\n",
            "    await client.close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_generated/_async_client.py\", line 3389, in close_session\n",
            "    for response in self._sessions[session]:\n",
            "                    ~~~~~~~~~~~~~~^^^^^^^^^\n",
            "KeyError: <aiohttp.client.ClientSession object at 0x7dc2b7dcbd10>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AgentOutput(response=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='<think>\\nOkay, the user wants me to search a database for \\'science fiction\\' and return some persona descriptions. Let me start by understanding what they need. They mentioned using a tool called personas, which I assume is a tool that provides descriptions for different types of personas. \\n\\nFirst, I need to check if the tool is available. The tool name is personas, and the description says it provides descriptions for various types of personas. The required input is a title, which in this case is \\'science fiction\\'. So I should use the personas tool with the input \\'science fiction\\'.\\n\\nNext, I need to make sure I\\'m using the correct tool and input. The user might be referring to a specific database, but since they didn\\'t specify, I\\'ll proceed with the assumption that the personas tool is designed to handle such queries. \\n\\nI should also consider if there are any additional parameters needed. The tool\\'s description mentions a properties object with input and type, so the input should be a string titled \\'Input\\'. The required field is \\'input\\', so the JSON should look like {\"input\": \"science fiction\"}.\\n\\nAfter running the tool, I\\'ll need to process the response. The tool might return a list of persona descriptions related to science fiction. Each description would')]), structured_response=None, current_agent_name='Agent', raw=ChatCompletionStreamOutput(choices=[ChatCompletionStreamOutputChoice(delta=ChatCompletionStreamOutputDelta(role=None, content=' would', tool_call_id=None, tool_calls=None), index=0, finish_reason=None, logprobs=None)], created=1753959149, id='chatcmpl-Root=1-688b4aed-7a6f727c7aee15092c8a6ef2', model='HuggingFaceTB/SmolLM3-3B', system_fingerprint=None, usage=None, object='chat.completion.chunk'), tool_calls=[], retry_messages=[])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "handler = query_engine_agent.run(\n",
        "    \"Search the database for 'science fiction' and return some persona descriptions.\"\n",
        ")\n",
        "async for ev in handler.stream_events():\n",
        "    if isinstance(ev, ToolCallResult):\n",
        "        print(\"\")\n",
        "        print(\"Called tool: \", ev.tool_name, ev.tool_kwargs, \"=>\", ev.tool_output)\n",
        "    elif isinstance(ev, AgentStream):  # showing the thought process\n",
        "        print(ev.delta, end=\"\", flush=True)\n",
        "\n",
        "resp = await handler\n",
        "resp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDs9WycB2y2S"
      },
      "source": [
        "## Creating multi-agent systems\n",
        "\n",
        "We can also create multi-agent systems by passing multiple agents to the `AgentWorkflow` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "8qXNmSEo2y2S"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.agent.workflow import (\n",
        "    AgentWorkflow,\n",
        "    ReActAgent,\n",
        ")\n",
        "\n",
        "\n",
        "# Define some tools\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Add two numbers.\"\"\"\n",
        "    return a + b\n",
        "\n",
        "\n",
        "def subtract(a: int, b: int) -> int:\n",
        "    \"\"\"Subtract two numbers.\"\"\"\n",
        "    return a - b\n",
        "\n",
        "\n",
        "# Create agent configs\n",
        "# NOTE: we can use FunctionAgent or ReActAgent here.\n",
        "# FunctionAgent works for LLMs with a function calling API.\n",
        "# ReActAgent works for any LLM.\n",
        "calculator_agent = ReActAgent(\n",
        "    name=\"calculator\",\n",
        "    description=\"Performs basic arithmetic operations\",\n",
        "    system_prompt=\"You are a calculator assistant. Use your tools for any math operation.\",\n",
        "    tools=[add, subtract],\n",
        "    llm=llm,\n",
        ")\n",
        "\n",
        "query_agent = ReActAgent(\n",
        "    name=\"info_lookup\",\n",
        "    description=\"Looks up information about XYZ\",\n",
        "    system_prompt=\"Use your tool to query a RAG system to answer information about XYZ\",\n",
        "    tools=[query_engine_tool],\n",
        "    llm=llm,\n",
        ")\n",
        "\n",
        "# Create and run the workflow\n",
        "agent = AgentWorkflow(agents=[calculator_agent, query_agent], root_agent=\"calculator\")\n",
        "\n",
        "# Run the system\n",
        "handler = agent.run(user_msg=\"Can you add 5 and 3?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZxz31Kb2y2T",
        "outputId": "b48b2985-4334-4a58-e058-ddfa3668cedb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<think>\n",
            "Okay, let's see. The user is asking to add 5 and 3. I need to figure out which tool to use here. The available tools are add, subtract, and handoff. Since the question is about adding two numbers, the add tool should be the right choice.\n",
            "\n",
            "First, I should check if the user's language is specified. The question is in English, so I don't need to handle a different language. Next, I need to call the add tool with the arguments a=5 and b=3. The action input should be a JSON object with those values. \n",
            "\n",
            "Wait, the tool's description says the add function takes two integers. The user provided 5 and 3, which are both integers, so that's good. I don't need to use handoff here because the question is straightforward and doesn't require looking up information or passing to another agent. \n",
            "\n",
            "So the steps are: use the add tool, pass 5 and 3 as arguments, and then based on the tool's response, I can provide the answer. Since the add tool is simple, the response should be 8. I can then present that as the answer without needing further tools. \n",
            "\n",
            "I need to make sure the JSON for"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AgentOutput(response=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text=\"<think>\\nOkay, let's see. The user is asking to add 5 and 3. I need to figure out which tool to use here. The available tools are add, subtract, and handoff. Since the question is about adding two numbers, the add tool should be the right choice.\\n\\nFirst, I should check if the user's language is specified. The question is in English, so I don't need to handle a different language. Next, I need to call the add tool with the arguments a=5 and b=3. The action input should be a JSON object with those values. \\n\\nWait, the tool's description says the add function takes two integers. The user provided 5 and 3, which are both integers, so that's good. I don't need to use handoff here because the question is straightforward and doesn't require looking up information or passing to another agent. \\n\\nSo the steps are: use the add tool, pass 5 and 3 as arguments, and then based on the tool's response, I can provide the answer. Since the add tool is simple, the response should be 8. I can then present that as the answer without needing further tools. \\n\\nI need to make sure the JSON for\")]), structured_response=None, current_agent_name='calculator', raw=ChatCompletionStreamOutput(choices=[ChatCompletionStreamOutputChoice(delta=ChatCompletionStreamOutputDelta(role=None, content=' for', tool_call_id=None, tool_calls=None), index=0, finish_reason=None, logprobs=None)], created=1753959185, id='chatcmpl-Root=1-688b4b11-0491523c34e25e9b21dd7795', model='HuggingFaceTB/SmolLM3-3B', system_fingerprint=None, usage=None, object='chat.completion.chunk'), tool_calls=[], retry_messages=[])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "async for ev in handler.stream_events():\n",
        "    if isinstance(ev, ToolCallResult):\n",
        "        print(\"\")\n",
        "        print(\"Called tool: \", ev.tool_name, ev.tool_kwargs, \"=>\", ev.tool_output)\n",
        "    elif isinstance(ev, AgentStream):  # showing the thought process\n",
        "        print(ev.delta, end=\"\", flush=True)\n",
        "\n",
        "resp = await handler\n",
        "resp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVoCzAJ4HU_0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}