{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZrYfU7Xuofk"
      },
      "source": [
        "# Tools in LlamaIndex\n",
        "\n",
        "## Let's install the dependencies\n",
        "\n",
        "We will install the dependencies for this unit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Up1Zzi0Zuofo",
        "outputId": "77f3f0d8-0a13-4680-94b4-164afbee9cd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m111.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.2/313.2 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m115.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m97.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m102.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m111.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.1/137.1 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install llama-index llama-index-vector-stores-chroma llama-index-llms-huggingface-api llama-index-embeddings-huggingface llama-index-tools-google -U -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVkK4ABYuofp"
      },
      "source": [
        "And, let's log in to Hugging Face to use serverless Inference APIs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "wK-KsEiEuofp",
        "outputId": "f65f4b55-68c6-4899-a3fc-65f0fe4de45e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLwCouJiuofq"
      },
      "source": [
        "## Creating a FunctionTool\n",
        "\n",
        "Let's create a basic `FunctionTool` and call it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmqncH8Xuofq",
        "outputId": "b9197254-8c96-4fc7-c4bc-036af9213fc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting weather for Beirut\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ToolOutput(blocks=[TextBlock(block_type='text', text='The weather in Beirut is sunny')], tool_name='my_weather_tool', raw_input={'args': ('Beirut',), 'kwargs': {}}, raw_output='The weather in Beirut is sunny', is_error=False)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from llama_index.core.tools import FunctionTool\n",
        "\n",
        "\n",
        "def get_weather(location: str) -> str:\n",
        "    \"\"\"Useful for getting the weather for a given location.\"\"\"\n",
        "    print(f\"Getting weather for {location}\")\n",
        "    return f\"The weather in {location} is sunny\"\n",
        "\n",
        "\n",
        "tool = FunctionTool.from_defaults(\n",
        "    get_weather,\n",
        "    name=\"my_weather_tool\",\n",
        "    description=\"Useful for getting the weather for a given location.\",\n",
        ")\n",
        "tool.call(\"Beirut\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Vg6CPUpuofq"
      },
      "source": [
        "## Creating a QueryEngineTool\n",
        "\n",
        "Let's now re-use the `QueryEngine` we defined in the [previous unit on tools](/tools.ipynb) and convert it into a `QueryEngineTool`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113
        },
        "id": "EH0zstW6wcoX",
        "outputId": "db2623ea-bcb4-474b-98fc-cd016f57dcb2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/618 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "data/train-00000-of-00001.parquet:   0%|          | 0.00/35.0M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from pathlib import Path\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "dataset = load_dataset(path=\"dvilasuero/finepersonas-v0.1-tiny\", split=\"train\")\n",
        "\n",
        "Path(\"data\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for i, persona in enumerate(dataset):\n",
        "    with open(Path(\"data\") / f\"persona_{i}.txt\", \"w\") as f:\n",
        "        f.write(persona[\"persona\"])\n",
        "\n",
        "\n",
        "reader = SimpleDirectoryReader(input_dir=\"data\")\n",
        "documents = reader.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnFHTIFDuofq",
        "outputId": "f10186ea-6b11-4bfc-a3d2-d379668a47d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ToolOutput(blocks=[TextBlock(block_type='text', text=\"<think>\\nOkay, let's see. The user is asking about research on the impact of AI on the future of work and society. The context provided includes two personas: one is an environmental sustainability-focused AI researcher who uses AI to tackle environmental challenges, and the other is a machine learning researcher in NLP and AI ethics, focusing on AI prompts for performance and ethics.\\n\\nHmm, the first persona is more about environmental sustainability, so their research might focus on how AI can help in that area, like optimizing energy use or monitoring ecosystems. The second persona is into NLP and ethics, so their work might involve how AI can be used in decision-making processes, maybe in policy or communication, and ensuring that AI systems are ethical and fair.\\n\\nThe user's query is about the impact on the future of work and society. Both personas could contribute to that. The environmental researcher might look at how AI can create new jobs in sustainability sectors or disrupt existing ones. The NLP researcher might explore how AI can enhance productivity in various industries, leading to changes in job roles. Also, both could address ethical considerations, like job displacement or the need for retraining.\\n\\nBut the context doesn't mention specific studies or papers. The answer should be general, based on the given personas. So,\")], tool_name='some useful name', raw_input={'input': 'Responds about research on the impact of AI on the future of work and society?'}, raw_output=Response(response=\"<think>\\nOkay, let's see. The user is asking about research on the impact of AI on the future of work and society. The context provided includes two personas: one is an environmental sustainability-focused AI researcher who uses AI to tackle environmental challenges, and the other is a machine learning researcher in NLP and AI ethics, focusing on AI prompts for performance and ethics.\\n\\nHmm, the first persona is more about environmental sustainability, so their research might focus on how AI can help in that area, like optimizing energy use or monitoring ecosystems. The second persona is into NLP and ethics, so their work might involve how AI can be used in decision-making processes, maybe in policy or communication, and ensuring that AI systems are ethical and fair.\\n\\nThe user's query is about the impact on the future of work and society. Both personas could contribute to that. The environmental researcher might look at how AI can create new jobs in sustainability sectors or disrupt existing ones. The NLP researcher might explore how AI can enhance productivity in various industries, leading to changes in job roles. Also, both could address ethical considerations, like job displacement or the need for retraining.\\n\\nBut the context doesn't mention specific studies or papers. The answer should be general, based on the given personas. So,\", source_nodes=[NodeWithScore(node=TextNode(id_='b2ed6bc4-ddd0-47d2-9feb-73d096f4af50', embedding=None, metadata={'file_path': '/content/data/persona_1896.txt', 'file_name': 'persona_1896.txt', 'file_type': 'text/plain', 'file_size': 211, 'creation_date': '2025-07-31', 'last_modified_date': '2025-07-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f669f139-d123-42b7-997e-e5a467b2a4c9', node_type='4', metadata={'file_path': '/content/data/persona_1896.txt', 'file_name': 'persona_1896.txt', 'file_type': 'text/plain', 'file_size': 211, 'creation_date': '2025-07-31', 'last_modified_date': '2025-07-31'}, hash='4e294386c5d020e668ef4f1ace0ce1e8797fc79c9d263eea0335fd51b020731d')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='An environmental sustainability-focused AI researcher or specialist who advocates for the integration of artificial intelligence in tackling global environmental challenges and mitigating its own climate impact.', mimetype='text/plain', start_char_idx=0, end_char_idx=211, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.649036290219179), NodeWithScore(node=TextNode(id_='e6124ff4-5398-4ac6-83dc-fc367c336872', embedding=None, metadata={'file_path': '/content/data/persona_1587.txt', 'file_name': 'persona_1587.txt', 'file_type': 'text/plain', 'file_size': 215, 'creation_date': '2025-07-31', 'last_modified_date': '2025-07-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='024e279c-3a3e-4858-9f49-8ef29a036452', node_type='4', metadata={'file_path': '/content/data/persona_1587.txt', 'file_name': 'persona_1587.txt', 'file_type': 'text/plain', 'file_size': 215, 'creation_date': '2025-07-31', 'last_modified_date': '2025-07-31'}, hash='a967a4bbeccc3c0ba94a78b63175d2f5ec6dc9edf7ff1cebb35c5cd4c469b82f')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='A machine learning researcher focused on natural language processing and artificial intelligence, with a strong emphasis on developing and utilizing AI prompts to enhance AI performance, decision-making, and ethics.', mimetype='text/plain', start_char_idx=0, end_char_idx=215, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.6430019035314183)], metadata={'b2ed6bc4-ddd0-47d2-9feb-73d096f4af50': {'file_path': '/content/data/persona_1896.txt', 'file_name': 'persona_1896.txt', 'file_type': 'text/plain', 'file_size': 211, 'creation_date': '2025-07-31', 'last_modified_date': '2025-07-31'}, 'e6124ff4-5398-4ac6-83dc-fc367c336872': {'file_path': '/content/data/persona_1587.txt', 'file_name': 'persona_1587.txt', 'file_type': 'text/plain', 'file_size': 215, 'creation_date': '2025-07-31', 'last_modified_date': '2025-07-31'}}), is_error=False)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import chromadb\n",
        "\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "\n",
        "db = chromadb.PersistentClient(path=\"./alfred_chroma_db\")\n",
        "chroma_collection = db.get_or_create_collection(\"alfred\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "# llm = HuggingFaceInferenceAPI(model_name=\"meta-llama/Llama-3.2-3B-Instruct\")\n",
        "llm = HuggingFaceInferenceAPI(model_name=\"HuggingFaceTB/SmolLM3-3B\")\n",
        "# index = VectorStoreIndex.from_vector_store(\n",
        "#     vector_store=vector_store, embed_model=embed_model\n",
        "# )\n",
        "\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    embed_model=embed_model,\n",
        "    vector_store=vector_store\n",
        ")\n",
        "\n",
        "query_engine = index.as_query_engine(llm=llm)\n",
        "tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=query_engine,\n",
        "    name=\"some useful name\",\n",
        "    description=\"some useful description\",\n",
        ")\n",
        "await tool.acall(\n",
        "    \"Responds about research on the impact of AI on the future of work and society?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHz0kBT30N5u"
      },
      "source": [
        "or"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5zujO_Y0YXt",
        "outputId": "e541a397-24fc-4b4b-89c3-24916dc388dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ToolOutput(blocks=[TextBlock(block_type='text', text=\"<think>\\nOkay, let's see. I need to summarize what the two personas say about AI's impact on work and society. First, I'll look at the first persona: an IT consultant or technology writer focused on analyzing the social implications and applications of AI. So, they probably talk about how AI affects society in terms of social implications and its applications. Maybe they discuss things like job displacement, new job creation, ethical considerations, and how AI is integrated into different sectors.\\n\\nThe second persona is a machine learning researcher in NLP and AI, with a focus on developing AI prompts to enhance AI performance, decision-making, and ethics. This person might be more technical, focusing on the technical aspects of AI. They might talk about how AI can be improved through better prompts, the ethical issues involved in AI development, and how AI can make better decisions or perform tasks more effectively. They might also discuss the societal impact through the lens of AI's capabilities and the need for ethical guidelines.\\n\\nPutting this together, the first persona's summary would cover the broader societal and work-related impacts, including both positive and negative aspects. The second persona would focus more on the technical advancements and ethical considerations in AI development. I need to make sure not to reference the context directly but to summarize their\")], tool_name='AIImpactTool', raw_input={'input': \"Summarize what the personas say about AI's impact on work and society.\"}, raw_output=Response(response=\"<think>\\nOkay, let's see. I need to summarize what the two personas say about AI's impact on work and society. First, I'll look at the first persona: an IT consultant or technology writer focused on analyzing the social implications and applications of AI. So, they probably talk about how AI affects society in terms of social implications and its applications. Maybe they discuss things like job displacement, new job creation, ethical considerations, and how AI is integrated into different sectors.\\n\\nThe second persona is a machine learning researcher in NLP and AI, with a focus on developing AI prompts to enhance AI performance, decision-making, and ethics. This person might be more technical, focusing on the technical aspects of AI. They might talk about how AI can be improved through better prompts, the ethical issues involved in AI development, and how AI can make better decisions or perform tasks more effectively. They might also discuss the societal impact through the lens of AI's capabilities and the need for ethical guidelines.\\n\\nPutting this together, the first persona's summary would cover the broader societal and work-related impacts, including both positive and negative aspects. The second persona would focus more on the technical advancements and ethical considerations in AI development. I need to make sure not to reference the context directly but to summarize their\", source_nodes=[NodeWithScore(node=TextNode(id_='1a7bb8c2-c937-4e40-9c42-d4ce9128c207', embedding=None, metadata={'file_path': '/content/data/persona_601.txt', 'file_name': 'persona_601.txt', 'file_type': 'text/plain', 'file_size': 129, 'creation_date': '2025-07-31', 'last_modified_date': '2025-07-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='23f98a41-7aa7-43e8-8b6a-c428612d3c7e', node_type='4', metadata={'file_path': '/content/data/persona_601.txt', 'file_name': 'persona_601.txt', 'file_type': 'text/plain', 'file_size': 129, 'creation_date': '2025-07-31', 'last_modified_date': '2025-07-31'}, hash='46789058863cc72656a717b2dcc57f3273e4be4fc99ede3ebee11707c8105391')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='An IT consultant or a technology writer focused on analyzing the social implications and applications of artificial intelligence.', mimetype='text/plain', start_char_idx=0, end_char_idx=129, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.7044021447636877), NodeWithScore(node=TextNode(id_='e6124ff4-5398-4ac6-83dc-fc367c336872', embedding=None, metadata={'file_path': '/content/data/persona_1587.txt', 'file_name': 'persona_1587.txt', 'file_type': 'text/plain', 'file_size': 215, 'creation_date': '2025-07-31', 'last_modified_date': '2025-07-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='024e279c-3a3e-4858-9f49-8ef29a036452', node_type='4', metadata={'file_path': '/content/data/persona_1587.txt', 'file_name': 'persona_1587.txt', 'file_type': 'text/plain', 'file_size': 215, 'creation_date': '2025-07-31', 'last_modified_date': '2025-07-31'}, hash='a967a4bbeccc3c0ba94a78b63175d2f5ec6dc9edf7ff1cebb35c5cd4c469b82f')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='A machine learning researcher focused on natural language processing and artificial intelligence, with a strong emphasis on developing and utilizing AI prompts to enhance AI performance, decision-making, and ethics.', mimetype='text/plain', start_char_idx=0, end_char_idx=215, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.701271785624874)], metadata={'1a7bb8c2-c937-4e40-9c42-d4ce9128c207': {'file_path': '/content/data/persona_601.txt', 'file_name': 'persona_601.txt', 'file_type': 'text/plain', 'file_size': 129, 'creation_date': '2025-07-31', 'last_modified_date': '2025-07-31'}, 'e6124ff4-5398-4ac6-83dc-fc367c336872': {'file_path': '/content/data/persona_1587.txt', 'file_name': 'persona_1587.txt', 'file_type': 'text/plain', 'file_size': 215, 'creation_date': '2025-07-31', 'last_modified_date': '2025-07-31'}}), is_error=False)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=query_engine,\n",
        "    name=\"AIImpactTool\",\n",
        "    description=\"Answers questions using retrieved persona research; avoid meta-thinking, give concise answers.\"\n",
        ")\n",
        "await tool.acall(\"Summarize what the personas say about AI's impact on work and society.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDYoB9--uofr"
      },
      "source": [
        "## Creating Toolspecs\n",
        "\n",
        "Let's create a `ToolSpec` from the `GmailToolSpec` from the LlamaHub and convert it to a list of tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DfLdrkUuofs",
        "outputId": "8f9b7e19-8824-4e6f-af2f-798bb5a9790d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<llama_index.core.tools.function_tool.FunctionTool at 0x7e3b4493b450>,\n",
              " <llama_index.core.tools.function_tool.FunctionTool at 0x7e3b4493a910>,\n",
              " <llama_index.core.tools.function_tool.FunctionTool at 0x7e3b19dc4f10>,\n",
              " <llama_index.core.tools.function_tool.FunctionTool at 0x7e3b3aacb890>,\n",
              " <llama_index.core.tools.function_tool.FunctionTool at 0x7e3b3aac8f90>,\n",
              " <llama_index.core.tools.function_tool.FunctionTool at 0x7e3b1b864190>]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from llama_index.tools.google import GmailToolSpec\n",
        "\n",
        "tool_spec = GmailToolSpec()\n",
        "tool_spec_list = tool_spec.to_tool_list()\n",
        "tool_spec_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NfyxBlIuofs"
      },
      "source": [
        "To get a more detailed view of the tools, we can take a look at the `metadata` of each tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYbxA5C2uofs",
        "outputId": "32fc15db-759f-408f-87f2-e14fb85de751"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "load_data load_data() -> List[llama_index.core.schema.Document]\n",
            "Load emails from the user's account.\n",
            "search_messages search_messages(query: str, max_results: Optional[int] = None)\n",
            "\n",
            "        Searches email messages given a query string and the maximum number\n",
            "        of results requested by the user\n",
            "           Returns: List of relevant message objects up to the maximum number of results.\n",
            "\n",
            "        Args:\n",
            "            query (str): The user's query\n",
            "            max_results (Optional[int]): The maximum number of search results\n",
            "            to return.\n",
            "\n",
            "        \n",
            "create_draft create_draft(to: Optional[List[str]] = None, subject: Optional[str] = None, message: Optional[str] = None) -> str\n",
            "\n",
            "        Create and insert a draft email.\n",
            "           Print the returned draft's message and id.\n",
            "           Returns: Draft object, including draft id and message meta data.\n",
            "\n",
            "        Args:\n",
            "            to (Optional[str]): The email addresses to send the message to\n",
            "            subject (Optional[str]): The subject for the event\n",
            "            message (Optional[str]): The message for the event\n",
            "\n",
            "        \n",
            "update_draft update_draft(to: Optional[List[str]] = None, subject: Optional[str] = None, message: Optional[str] = None, draft_id: str = None) -> str\n",
            "\n",
            "        Update a draft email.\n",
            "           Print the returned draft's message and id.\n",
            "           This function is required to be passed a draft_id that is obtained when creating messages\n",
            "           Returns: Draft object, including draft id and message meta data.\n",
            "\n",
            "        Args:\n",
            "            to (Optional[str]): The email addresses to send the message to\n",
            "            subject (Optional[str]): The subject for the event\n",
            "            message (Optional[str]): The message for the event\n",
            "            draft_id (str): the id of the draft to be updated\n",
            "\n",
            "        \n",
            "get_draft get_draft(draft_id: str = None) -> str\n",
            "\n",
            "        Get a draft email.\n",
            "           Print the returned draft's message and id.\n",
            "           Returns: Draft object, including draft id and message meta data.\n",
            "\n",
            "        Args:\n",
            "            draft_id (str): the id of the draft to be updated\n",
            "\n",
            "        \n",
            "send_draft send_draft(draft_id: str = None) -> str\n",
            "\n",
            "        Sends a draft email.\n",
            "           Print the returned draft's message and id.\n",
            "           Returns: Draft object, including draft id and message meta data.\n",
            "\n",
            "        Args:\n",
            "            draft_id (str): the id of the draft to be updated\n",
            "\n",
            "        \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[None, None, None, None, None, None]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[print(tool.metadata.name, tool.metadata.description) for tool in tool_spec_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5B9yxhV10qVA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}